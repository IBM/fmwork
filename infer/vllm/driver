#!/usr/bin/env -S python -u

import argparse
import fmwork
import torch
import traceback
import vllm
#from rocm_mad_profiler import MADProfiler

class var: pass
class par: pass

def main():

    params()

    if par.dataset_path:
        fmwork.banner('DATASET')
        t0 = fmwork.time_get()
        fmwork.process_dataset(
            par.dataset_path, par.dataset_format, par.dataset_mode,
            par.model_path)
        t1 = fmwork.time_get()
        print(); print('FMWORK DATASET', '%.6f' % (fmwork.time_diff(t1, t0)))

    llm()
    runs()
    done()

def params():

    fmwork.banner('PARAMS')

    parser = argparse.ArgumentParser()

    parser.add_argument('-m', '--model_path',
        type=str, required=True)
    parser.add_argument(      '--dataset_format',
        type=str, default='json', choices=['json', 'processed'])
    parser.add_argument(      '--dataset_path',
        type=str)
    parser.add_argument(      '--dataset_mode',
        type=str, default='expand', choices=['expand', 'real'])
    parser.add_argument('-i', '--input_size',
        type=str, required=True)
    parser.add_argument('-o', '--output_size',
        type=str, required=True)
    parser.add_argument('-b', '--batch_size',
        type=str, required=True)
    parser.add_argument('-t', '--tensor_parallel',
        type=int, required=True)
    parser.add_argument('-r', '--reps',
        type=int, default=3)
    parser.add_argument('-d', '--dtype',
        type=str, default='auto')
    parser.add_argument('-q', '--quantization',
        type=str, default=None)
    parser.add_argument('-k', '--kv_cache_dtype',
        type=str, default='auto')
    parser.add_argument('-u', '--gpu_memory_utilization',
        type=float, default=0.95)
    parser.add_argument('-e', '--enforce_eager',
        action='store_true')
    parser.add_argument('-s', '--num_scheduler_steps',
        type=int, default=1)
    parser.add_argument(      '--use_v2_block_manager',
        action='store_true')
    parser.add_argument(      '--distributed_executor_backend',
        default='mp')
    parser.add_argument('-M', '--max_num_seqs',
        type=int, default=512)
    parser.add_argument('-C', '--enable_chunked_prefill',
        type=str)
    parser.add_argument('--id', type=str)
    parser.add_argument('--show_reps', action='store_true')
    parser.add_argument('--ignore_eos', type=str, default='True')
    parser.add_argument('--debug_outputs', action='store_true')
    parser.add_argument('--trace_outputs', action='store_true')
    parser.add_argument('--stop_itl', type=float)
    parser.add_argument('--enable_prefix_caching', action='store_true')
    parser.add_argument('--max_seq_len_to_capture', type=int)
    parser.add_argument('--max_num_batched_tokens', type=int)

    parser.parse_args(namespace=par)

    attrs = []
    for attr in dir(par):
        if not attr.startswith('__') and not attr.endswith('__'):
            attrs.append(attr)
    pad = max([len(x) for x in attrs])
    for attr in sorted(attrs):
        print('%-*s = %s' % (
            pad, attr, getattr(par, attr)))

    var.input_sizes  = list(map(int, par.input_size.split(',')))
    var.output_sizes = list(map(int, par.output_size.split(',')))
    var.batch_sizes  = list(map(int, par.batch_size.split(',')))

    if par.quantization == 'None': par.quantization = None

    if not par.enable_chunked_prefill: par.enable_chunked_prefill = False
    else: par.enable_chunked_prefill = \
        par.enable_chunked_prefill == 'True' or \
        par.enable_chunked_prefill == '1'

    par.ignore_eos = par.ignore_eos == 'True' or \
        par.ignore_eos == '1'

def llm():

    fmwork.banner('LLM')

    t0 = fmwork.time_get()

    # Include these parameters only if set; otherwise let internal vLLM logic set them.
    optional_params = {"max_seq_len_to_capture", "max_num_batched_tokens"}
    kwargs = {param: getattr(par, param) for param in optional_params if getattr(par, param) is not None}

    var.llm = vllm.LLM(
        dtype                        = par.dtype,
        enforce_eager                = par.enforce_eager,
        gpu_memory_utilization       = par.gpu_memory_utilization,
        kv_cache_dtype               = par.kv_cache_dtype,
        max_model_len                = max(var.input_sizes) + max(var.output_sizes),
        model                        = par.model_path,
        quantization                 = par.quantization,
        tensor_parallel_size         = par.tensor_parallel,
        trust_remote_code            = True,
        num_scheduler_steps          = par.num_scheduler_steps,
        use_v2_block_manager         = par.use_v2_block_manager,
        distributed_executor_backend = par.distributed_executor_backend,
        max_num_seqs                 = par.max_num_seqs,
        enable_chunked_prefill       = par.enable_chunked_prefill,
        enable_prefix_caching        = par.enable_prefix_caching,
        **kwargs
    )

    t1 = fmwork.time_get()

    print(); print('FMWORK SETUP', '%.6f' % (fmwork.time_diff(t1, t0)))

def runs():

    oo1 = {}

    for batch_size in var.batch_sizes:
        for input_size in var.input_sizes:
            for output_size in var.output_sizes:
                etim, med = run(input_size, output_size, batch_size)

                if output_size == 1:
                    if input_size not in oo1: oo1[input_size] = {}
                    oo1[input_size][batch_size] = med
                else:
                    if input_size not in oo1: continue
                    if batch_size not in oo1[input_size]: continue
                    ttft = oo1[input_size][batch_size]
                    gen  = med - ttft
                    itl  = gen / output_size * 1000.0 # ms
                    thp  = batch_size * output_size / gen

                    print(
                        'FMWORK GEN',
                        etim,
                        input_size,
                        output_size,
                        batch_size,
                        par.tensor_parallel,
                        '%.3f' % (med),
                        '%.3f' % (gen),
                        '%.3f' % (ttft),
                        '%.1f' % (itl),
                        '%.1f' % (thp),
                    )

                    print()
                    print('TTFT / GEN Metrics')
                    print('------------------')
                    print()
                    print('Experiment timestamp:            %s'   % (etim))
                    print('Model path:                      %s'   % (par.model_path))
                    print('Input size:                      %d'   % (input_size))
                    print('Output size:                     %d'   % (output_size))
                    print('Batch size:                      %d'   % (batch_size))
                    print('Tensor parallel size:            %d'   % (par.tensor_parallel))
                    print('MED  - Median iteration (s):     %.3f' % (med))
                    print('TTFT - Time to first token (s):  %.3f' % (ttft))
                    print('GEN  - Generation time (s):      %.3f' % (gen))
                    print('ITL  - Inter-token latency (ms): %.1f' % (itl))
                    print('THP  - Throughput (tok/s):       %.1f' % (thp))

                    if par.stop_itl:
                        if itl > float(par.stop_itl):
                            print(); print(
                                'Stopping at ITL >',
                                '%.1f' % (par.stop_itl))
                            return

def run(input_size, output_size, batch_size):

    fmwork.banner(
        'RUN',
        input_size,  '/',
        output_size, '/',
        batch_size,  '/',
        par.tensor_parallel
    )

    if not par.dataset_path:

        input_batch = fmwork.input_generator(
            par.model_path,
            input_size, batch_size,
            return_tensors='np')

    else:

        input_batch = fmwork.input_dataset(
            par.model_path,
            par.dataset_mode,
            input_size, batch_size)

    sampling_params = vllm.SamplingParams(
        max_tokens = output_size,
        ignore_eos = par.ignore_eos
    )

    kwargs = {
        'prompt_token_ids' : input_batch,
        'sampling_params'  : sampling_params,
        'use_tqdm'         : False,
    }

    fmwork.reset()

    for rep in range(par.reps):
        input_batch = fmwork.input_generator(par.model_path, input_size, batch_size, return_tensors='np')
        kwargs['prompt_token_ids'] = input_batch
        fmwork.t0()
        #with MADProfiler(backend="rpd", nvtx_tracing=True):
        #    outputs = var.llm.generate(**kwargs)
        outputs = var.llm.generate(**kwargs)
        torch.cuda.synchronize()
        ret = fmwork.t1(
            rep, par.reps,
            input_size, output_size, batch_size,
            par.tensor_parallel,
            par.ignore_eos, outputs,
            par.debug_outputs, par.trace_outputs)

    return ret

def done():

    fmwork.banner('DONE')

if __name__ == '__main__':
    try: main()
    except SystemExit: pass
    except: traceback.print_exc()

