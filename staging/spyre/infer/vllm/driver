#!/usr/bin/env -S python -u

import argparse
import fmwork
import gc
import os
import torch
import traceback
import vllm
import time
class var: pass
class par: pass

def main():

    params()

    # spyre environment variables
    max_input = max(var.input_sizes)
    repeated_input = ",".join([str(max_input)] * len(var.batch_sizes))
    os.environ["VLLM_SPYRE_WARMUP_PROMPT_LENS"] = repeated_input
    max_output = max(var.output_sizes)
    repeated_output = ",".join([str(max_output)] * len(var.batch_sizes))
    os.environ["VLLM_SPYRE_WARMUP_NEW_TOKENS"] = repeated_output
    os.environ['VLLM_SPYRE_WARMUP_BATCH_SIZES'] = par.batch_size

    if par.dataset_path:
        fmwork.banner('DATASET')
        t0 = fmwork.time_get()
        fmwork.process_dataset(
            par.dataset_path, par.dataset_format, par.dataset_mode,
            par.model_path)
        t1 = fmwork.time_get()
        print(); print('FMWORK DATASET', '%.6f' % (fmwork.time_diff(t1, t0)))

    llm()
    runs()
    done()

def params():

    fmwork.banner('PARAMS')

    parser = argparse.ArgumentParser()

    parser.add_argument('-m', '--model_path',
        type=str, required=True)
    parser.add_argument(      '--dataset_format',
        type=str, default='json', choices=['json', 'processed'])
    parser.add_argument(      '--dataset_path',
        type=str)
    parser.add_argument(      '--dataset_mode',
        type=str, default='expand', choices=['expand', 'real'])
    parser.add_argument('-i', '--input_size',
        type=str, required=True)
    parser.add_argument('-o', '--output_size',
        type=str, required=True)
    parser.add_argument('-b', '--batch_size',
        type=str, required=True)
    parser.add_argument('-t', '--tensor_parallel',
        type=int, required=True)
    parser.add_argument('-r', '--reps',
        type=int, default=3)
    parser.add_argument('-d', '--dtype',
        type=str, default='auto')
    parser.add_argument('-q', '--quantization',
        type=str, default=None)
    parser.add_argument('-k', '--kv_cache_dtype',
        type=str, default='auto')
    parser.add_argument('-u', '--gpu_memory_utilization',
        type=float, default=0.95)
    parser.add_argument('-e', '--enforce_eager',
        action='store_true')
    parser.add_argument('-s', '--num_scheduler_steps',
        type=int, default=1)
    parser.add_argument(      '--use_v2_block_manager',
        action='store_true')
    parser.add_argument(      '--distributed_executor_backend',
        default='mp')
    parser.add_argument('-M', '--max_num_seqs',
        type=int, default=512)
    parser.add_argument('-C', '--enable_chunked_prefill',
        type=str)
    parser.add_argument('--id', type=str)
    parser.add_argument('--show_reps', action='store_true')
    parser.add_argument('--ignore_eos', type=str, default='True')
    parser.add_argument('--debug_outputs', action='store_true')
    parser.add_argument('--trace_outputs', action='store_true')
    parser.add_argument('--stop_itl', type=float)
    parser.add_argument('-bt', '--batch_size_times',
        type=int, default=1)
    parser.add_argument('--delay_per_iter', type=float, default=0,
        help='Time to sleep (in seconds) between iterations. Default is 0.')
    parser.add_argument('--enable_profiler',
        nargs='?',
        const='./vllm_profile',
        default=None,
        help='Enable torch profiler. Optionally specify output dir (default: ./vllm_profile)')

    parser.parse_args(namespace=par)

    attrs = []
    for attr in dir(par):
        if not attr.startswith('__') and not attr.endswith('__'):
            attrs.append(attr)
    pad = max([len(x) for x in attrs])
    for attr in sorted(attrs):
        print('%-*s = %s' % (
            pad, attr, getattr(par, attr)))

    var.input_sizes  = list(map(int, par.input_size.split(',')))
    var.output_sizes = list(map(int, par.output_size.split(',')))
    var.batch_sizes  = list(map(int, par.batch_size.split(',')))

    if par.quantization == 'None': par.quantization = None

    if not par.enable_chunked_prefill: par.enable_chunked_prefill = False
    else: par.enable_chunked_prefill = \
        par.enable_chunked_prefill == 'True' or \
        par.enable_chunked_prefill == '1'

    par.ignore_eos = par.ignore_eos == 'True' or \
        par.ignore_eos == '1'
    if par.enable_profiler:
        profiler_dir = par.enable_profiler
        try:
            os.makedirs(profiler_dir, exist_ok=True)
        except Exception as e:
            print(f"Warning: could not create profiler output dir ({profiler_dir}): {e}")
        os.environ["VLLM_TORCH_PROFILER_DIR"] = profiler_dir
        print(f"Profiler is ENABLED — trace will be saved to: {profiler_dir}")

def llm():

    fmwork.banner('LLM')

    t0 = fmwork.time_get()

    var.llm = vllm.LLM(
        dtype                        = par.dtype,
        enforce_eager                = par.enforce_eager,
        gpu_memory_utilization       = par.gpu_memory_utilization,
        kv_cache_dtype               = par.kv_cache_dtype,
        max_model_len                = max(var.input_sizes) + max(var.output_sizes),
        model                        = par.model_path,
        quantization                 = par.quantization,
        tensor_parallel_size         = par.tensor_parallel,
        trust_remote_code            = True,
        num_scheduler_steps          = par.num_scheduler_steps,
        use_v2_block_manager         = par.use_v2_block_manager,
        distributed_executor_backend = par.distributed_executor_backend,
        max_num_seqs                 = par.max_num_seqs,
        enable_chunked_prefill       = par.enable_chunked_prefill,

        enable_prefix_caching        = False
    )

    t1 = fmwork.time_get()

    print(); print('FMWORK SETUP', '%.6f' % (fmwork.time_diff(t1, t0)))

def runs():

    oo1 = {}

    for batch_size in var.batch_sizes:
        for input_size in var.input_sizes:
            for output_size in var.output_sizes:
                etim, med = run(input_size, output_size, batch_size * par.batch_size_times)

                if output_size == 1:
                    if input_size not in oo1: oo1[input_size] = {}
                    oo1[input_size][batch_size] = med
                else:
                    if input_size not in oo1: continue
                    if batch_size not in oo1[input_size]: continue
                    ttft = oo1[input_size][batch_size]
                    gen  = med - ttft
                    itl  = gen / output_size * 1000.0 # ms
                    thp  = batch_size * output_size / gen

                    print(
                        'FMWORK GEN',
                        etim,
                        input_size,
                        output_size,
                        batch_size,
                        par.tensor_parallel,
                        '%.3f' % (med),
                        '%.3f' % (gen),
                        '%.3f' % (ttft),
                        '%.1f' % (itl),
                        '%.1f' % (thp),
                    )

                    print()
                    print('TTFT / GEN Metrics')
                    print('------------------')
                    print()
                    print('Experiment timestamp:            %s'   % (etim))
                    print('Model path:                      %s'   % (par.model_path))
                    print('Input size:                      %d'   % (input_size))
                    print('Output size:                     %d'   % (output_size))
                    print('Batch size:                      %d'   % (batch_size))
                    print('Tensor parallel size:            %d'   % (par.tensor_parallel))
                    print('MED  - Median iteration (s):     %.3f' % (med))
                    print('TTFT - Time to first token (s):  %.3f' % (ttft))
                    print('GEN  - Generation time (s):      %.3f' % (gen))
                    print('ITL  - Inter-token latency (ms): %.1f' % (itl))
                    print('THP  - Throughput (tok/s):       %.1f' % (thp))
                    print(f'# Note: runtime averaged over batch_size_times={par.batch_size_times}')
                    print(f'# The first (and last) dequeue of the task queue isn‘t a full batch.')
                    print(f'# To make it deterministic, for a specific batch size (e.g., {batch_size}),')
                    print(f'# feed batch_size * batch_size_times = {batch_size * par.batch_size_times} samples')
                    print(f'# to saturate the engine and amortize the efficiency lost in the first and last batch.')
                    print()

                    print('Settings for Spyre')
                    print('------------------')
                    print()
                    print('OMP_NUM_THREADS:                 %d'   % (torch.get_num_threads()))
                    print('FLEX_HDMA_MODE_FULL:             %s'   % (os.environ.get("FLEX_HDMA_MODE_FULL")))
                    print('FLEX_RDMA_MODE_FULL:             %s'   % (os.environ.get("FLEX_RDMA_MODE_FULL")))
                    print('COLL_ALLREDUCE_ALGO:             %s'   % (os.environ.get("COLL_ALLREDUCE_ALGO")))

                    if par.stop_itl:
                        if itl > float(par.stop_itl):
                            print(); print(
                                'Stopping at ITL >',
                                '%.1f' % (par.stop_itl))
                            return

def run(input_size, output_size, batch_size):

    fmwork.banner(
        'RUN',
        input_size,  '/',
        output_size, '/',
        batch_size,  '/',
        par.tensor_parallel
    )

    if not par.dataset_path:

        input_batch = fmwork.input_generator(
            par.model_path,
            input_size, batch_size,
            return_tensors='np')

    else:

        input_batch = fmwork.input_dataset(
            par.model_path,
            par.dataset_mode,
            input_size, batch_size)

    sampling_params = vllm.SamplingParams(
        max_tokens = output_size,
        ignore_eos = par.ignore_eos
    )

    kwargs = {
        'prompt_token_ids' : input_batch,
        'sampling_params'  : sampling_params,
        'use_tqdm'         : False,
    }

    fmwork.reset()

    for rep in range(par.reps):
        fmwork.t0()
        if par.enable_profiler and rep == 1:
            try:
                var.llm.start_profile()
                outputs = var.llm.generate(**kwargs)
                var.llm.stop_profile()
            except Exception as e:
                print("Profiler error:", e)
                outputs = var.llm.generate(**kwargs)
        else:
            outputs = var.llm.generate(**kwargs)
        try:
            # for cuda devices
            torch.cuda.synchronize()
        except:
            # for spyre
            pass
        ret = fmwork.t1(
            rep, par.reps,
            input_size, output_size, batch_size,
            par.tensor_parallel,
            par.ignore_eos, outputs,
            par.debug_outputs, par.trace_outputs,
            batch_size_times=par.batch_size_times)
        time.sleep(par.delay_per_iter)
    return ret

def done():

    fmwork.banner('DONE')
    # needed to prevent ugly stackdump caused by sigterm. Should be fixed in image.
    del var.llm
    gc.collect()


if __name__ == '__main__':
    try: main()
    except SystemExit: pass
    except: traceback.print_exc()
